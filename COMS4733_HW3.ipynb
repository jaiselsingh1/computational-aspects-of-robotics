{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMS 4733 Computational Aspects of Robotics -- Homework 3\n",
        "\n",
        "In this homework, we will go over the perception workflow of a robotic system: from camera calibration, to point cloud construction, segmentation, and semantic feature queries.\n",
        "\n",
        "**What's covered**\n",
        "\n",
        "- Pinhole camera model\n",
        "- Camera calibration\n",
        "- 3D coordinate transformation\n",
        "- RGB, depth, and robot trajectory data processing\n",
        "- Point cloud visualization\n",
        "- Segmentation mask processing\n",
        "- DINO feature extraction and similarity calculation\n",
        "\n",
        "**Data**\n",
        "\n",
        "Data can be downloaded from [Google Drive](https://drive.google.com/file/d/1QwiaJQWEGN_Kmvs_bOmcBlbauv8R7N2M/view?usp=sharing) (LionMail required for viewing), and should be extracted under the same directory as the .ipynb file. The extracted data structure should look like:\n",
        "- parent directory/\n",
        "  - COMS4733_HW3.ipynb\n",
        "  - data/\n",
        "    - calibration/\n",
        "    - recording/\n",
        "    - vis/\n",
        "\n",
        "**Submission**\n",
        "\n",
        "Submit a ```.zip``` file containing the following:\n",
        "- The notebook file (```.ipynb```) with completed codes and the outputs of the code blocks.\n",
        "- A ```.pdf``` report, containing answers to the written questions. All written questions can be found in the text instructions of each problem.\n",
        "\n",
        "**Discussion policy**\n",
        "\n",
        "You may discuss high-level ideas with classmates,\n",
        "but your answers must be your own. Cite all external sources. List collaborators\n",
        "(names/UNIs) at the end of the PDF report.\n",
        "\n",
        "**Accessibility and extensions**\n",
        "\n",
        "For documented accommodations or emergencies, contact the instructor/TA prior to the deadline when possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Notebook Setup\n",
        "\n",
        "We recommend running the code in your local environment. If you use a local environment, we recommend using [miniconda](https://www.anaconda.com/docs/getting-started/miniconda/main) or [uv](https://docs.astral.sh/uv/) for python environment management. Most packages can be installed via pip. \n",
        "\n",
        "**Special requirements**\n",
        "- For pytorch, please refer to the [official website](https://pytorch.org/get-started/locally/) for installation commands.\n",
        "- **important**: Install ```cv2``` via ```pip install opencv-contrib-python==4.12.0```. ```opencv-contrib-python``` and ```opencv-python``` are similar packages and share the name ```cv2```. However, only ```opencv-contrib-python``` contains the aruco marker detection functions. If you have ```opencv-python``` previously installed, please uninstall first.\n",
        "- ```open3d==0.19.0``` is recommended for better integration with jupyter notebook. When running open3d visualizations, i.e. ```o3d.visualization.visualize_geometries``` and ```o3d.visualization.Visualizer()```, GUI display will pop up.\n",
        "- If you have difficulties accessing the data, do not have a local GUI machine, or do not have enough CPU/RAM for the code, please report to the TAs.\n",
        "\n",
        "GPU is **not required** for this assignment.\n",
        "\n",
        "After you successfully set up your environment, run the following code to test that all packages can be imported correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "53c06c0d",
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import open3d as o3d\n",
        "import transforms3d\n",
        "import torch\n",
        "from torch.nn.functional import interpolate\n",
        "import time\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26e05b0",
      "metadata": {},
      "source": [
        "## 1. Camera Calibration (15 pts)\n",
        "\n",
        "### 1.1 Constructing the Intrinsic Matrix (5 pts)\n",
        "\n",
        "In this section, you will construct the 3x3 **camera intrinsics matrix** \\( K \\) using the given the camera parameters.\n",
        "\n",
        "You are provided with:\n",
        "- **Focal lengths** \\( f_x, f_y \\) (in pixels)\n",
        "- **Principal point** coordinates \\( c_x, c_y \\)\n",
        "\n",
        "**Instructions:** \n",
        "1. Implement the missing code. (3 pts)\n",
        "2. In the report, answer the following question: suppose we have an image of size H x W. For computational reasons, we want to downsample the image by half, i.e. if image is a numpy array with size (H, W, 3), we apply ```image = image[::2, ::2, :]```, resulting in a (H/2, W/2, 3) array. Now suppose the H x W image was originally captured by a pinhole camera with parameters ```fx, fy, px, py```. If we want to change the camera parameters, such that the captured image is directly the same with the downsampled H/2 x W/2 image ignoring differences within 1 pixel), how should we set the new ```fx, fy, cx, cy``` parameters? (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce752a85",
      "metadata": {
        "id": "helpers_camera"
      },
      "outputs": [],
      "source": [
        "def get_camera_intrinsics(fx: float, fy: float, cx: float, cy: float) -> np.ndarray:\n",
        "    # return: 3x3 camera intrinsics matrix K\n",
        "\n",
        "    ### TODO YOUR CODE STARTS ###\n",
        "    K = None\n",
        "    ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "    return K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f816c3",
      "metadata": {},
      "source": [
        "### 1.2 Charuco board detection (10 pts)\n",
        "\n",
        "Below we load the 4 camera images from `data/calibration/` and run Charuco/marker detection using OpenCV.\n",
        "We display an overlay for each camera showing detected markers and Charuco corners."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04151776",
      "metadata": {},
      "source": [
        "**Charuco board**\n",
        "\n",
        "We use charuco board for calibration. Charuco is a combination of Checkerboard and Aruco. A simple introduction is [here](https://docs.opencv.org/3.4/df/d4a/tutorial_charuco_detection.html). Below is a script that visualizes of the board."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2e34f504",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@46.395] global loadsave.cpp:275 findDecoder imread_('data/vis/charuco.jpg'): can't open/read file: check file path/integrity\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m board_image = cv2.imread(\u001b[33m'\u001b[39m\u001b[33mdata/vis/charuco.jpg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m plt.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plt.imshow(\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m plt.show()\n",
            "\u001b[31merror\u001b[39m: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "board_image = cv2.imread('data/vis/charuco.jpg')\n",
        "plt.axis('off')\n",
        "plt.imshow(cv2.cvtColor(board_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be9a274",
      "metadata": {},
      "source": [
        "The detection of charuco board is done by opencv. We use its python version. The opencv-python package (can be simply used by ```import cv2```) has a few predefined utilities we can use. For example, ```cv2.aruco.CharucoBoard``` defines the board parameters by taking in the size of the board (6x5 in our case), the edge length of the checkerboard (0.04m in our case), and the edge length of each aruco marker (0.03m in our case). Run the following code block to load the parameters and other utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1552d0ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# we first define the charuco board\n",
        "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
        "charuco_board = cv2.aruco.CharucoBoard((6, 5), 0.04, 0.03, aruco_dict)\n",
        "charuco_params = cv2.aruco.CharucoParameters()\n",
        "charuco_detector = cv2.aruco.CharucoDetector(charuco_board, charuco_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec08109",
      "metadata": {},
      "source": [
        "**Calibration function**\n",
        "\n",
        "Here we directly give the opencv function that takes in the image, detects charuco markers and visualize them, and finally returns ```rvec``` and ```tvec```, which represents the world-to-camera rotation and translation, represented in a 3-dimensional rotation vector and 3-dimensional translation vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1937de1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate(img_path, K):\n",
        "    # read image\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # detect charuco board\n",
        "    charuco_corners, charuco_ids, marker_corners, marker_ids = charuco_detector.detectBoard(gray)\n",
        "\n",
        "    vis = img.copy()\n",
        "    cv2.aruco.drawDetectedMarkers(vis, marker_corners, marker_ids)\n",
        "    plt.figure(figsize=(6, 4), dpi=150)\n",
        "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f'Detected markers')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # estimate pose\n",
        "    retval, rvec, tvec = cv2.aruco.estimatePoseCharucoBoard(\n",
        "        charuco_corners,\n",
        "        charuco_ids,\n",
        "        charuco_board,\n",
        "        cameraMatrix=K,\n",
        "        distCoeffs=np.zeros(5, dtype=np.float32),\n",
        "        rvec=None,  # placeholder\n",
        "        tvec=None,  # placeholder\n",
        "    )\n",
        "\n",
        "    # Visualize pose on the image\n",
        "    vis = img.copy()\n",
        "    cv2.drawFrameAxes(vis, K, np.zeros(5, dtype=np.float32), rvec, tvec, 0.1)\n",
        "    plt.figure(figsize=(6, 4), dpi=150)\n",
        "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Pose Visualization\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return rvec, tvec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8aa7407",
      "metadata": {},
      "source": [
        "Using the given functions, we can now estimate the world-to-camera transformations. Hint: ```cv2.Rodrigues``` is a useful function in converting 3D rotation vectors to the commonly-used 3x3 rotation matrix. Usage: ```rotation_matrix, _ = cv2.Rodrigues(rvec)```\n",
        "\n",
        "**Instructions:** \n",
        "1. Implement the missing code. (7 pts)\n",
        "2. In the report, answer the question: in the world frame, which axis is parallel to the vertical direction (orthogonal to the table surface)? On that axis, is the camera center's position positive or negative? (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83546998",
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['front', 'wrist']\n",
        "img_paths = ['./data/calibration/camera_front.png', './data/calibration/camera_wrist.png']\n",
        "\n",
        "# to store extrinsic matrices\n",
        "extrinsics = {}\n",
        "\n",
        "# read intrinsics file\n",
        "with open('data/calibration/intrinsics.json', 'r') as f:\n",
        "    intrinsics = json.load(f)\n",
        "\n",
        "for name, img_path in zip(names, img_paths):\n",
        "    # compute the calibration and get camera extrinsics\n",
        "\n",
        "    ### TODO YOUR CODE STARTS ###\n",
        "    world_T_camera = None\n",
        "    ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "    extrinsics[name] = world_T_camera.tolist()\n",
        "\n",
        "# save to file\n",
        "with open('data/calibration/extrinsics_result.json', 'w') as f:\n",
        "    json.dump(extrinsics, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786251c3",
      "metadata": {},
      "source": [
        "If you called the ```calibrate``` function correctly, you will be able to see the following visualizations (by running the script below; resolution may be different). For each camera view, the first visualization shows all the individual aruco markers detected (you can see green bounding boxes and blue texts displaying their ids); the second visualization shows the coordinate frame, located at one corner of the charuco board, and with red, green, and blue axes pointing towards the x, y, z directions of the world frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8e8b26",
      "metadata": {},
      "outputs": [],
      "source": [
        "board_image = cv2.imread('data/vis/calibration_result_1.png')\n",
        "plt.axis('off')\n",
        "plt.imshow(cv2.cvtColor(board_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "board_image = cv2.imread('data/vis/calibration_result_2.png')\n",
        "plt.axis('off')\n",
        "plt.imshow(cv2.cvtColor(board_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "board_image = cv2.imread('data/vis/calibration_result_3.png')\n",
        "plt.axis('off')\n",
        "plt.imshow(cv2.cvtColor(board_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "board_image = cv2.imread('data/vis/calibration_result_4.png')\n",
        "plt.axis('off')\n",
        "plt.imshow(cv2.cvtColor(board_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15fc14fe",
      "metadata": {},
      "source": [
        "If you calculated correctly, the saved extrinsics file ```data/calibration/extrinsics_result.json``` should be identical with ```data/calibration/extrinsics.json```. Note that we will check your code implementation for grading, but not the resulting file, so simply copying the file will not count."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0814c682",
      "metadata": {},
      "source": [
        "## 2. Point Cloud Visualization (45 pts)\n",
        "\n",
        "### 2.1 Inverse projection function (10 pts)\n",
        "\n",
        "Given the depth image and the intrinsics, how to recover the point cloud? This is equivalent to the inverse operation of the camera space to image plane projection, which we call \"inverse projection\". \n",
        "\n",
        "**Instructions**\n",
        "- Complete the inverse projection function. You may or may not use the provided ```x, y``` arrays generated using the ```np.meshgrid``` function. (7 pts)\n",
        "- In the report, write (as formulas) or briefly explain (in sentences) the mathmatical formulation of the inverse projection function. (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5191db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def depth_to_camera_frame_point_cloud(depth, K):\n",
        "    # depth: HxW depth image\n",
        "    # K: 3x3 camera intrinsics matrix\n",
        "    # return: Nx3 point cloud in the camera frame\n",
        "    H, W = depth.shape\n",
        "    x, y = np.meshgrid(np.arange(W), np.arange(H))\n",
        "\n",
        "    ### TODO YOUR CODE STARTS ###\n",
        "    points = None\n",
        "    ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "    return points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538855c8",
      "metadata": {},
      "source": [
        "### 2.2 Camera to world transformation function (10 pts)\n",
        "\n",
        "After we get the point clouds in the camera frame using the function in 2.1, we want to transform the points from the camera frame to the world frame (defined by the calibration charuco board). Suppose we are given the points and the extrinsic matrix, the function needs to perform the transformation and return the points in the world frame.\n",
        "\n",
        "**Instructions**\n",
        "- Complete the code. (7 pts)\n",
        "- In the report, write (as formulas) or briefly explain (in sentences) the mathmatical formulation of the transformation function. (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34cf7853",
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_camera_to_world(points, extrinsic_matrix):\n",
        "    # points: Nx3 point cloud in the camera frame\n",
        "    # extrinsic_matrix: 4x4 camera-to-world extrinsic matrix\n",
        "    # return: Nx3 point cloud in the world frame\n",
        "\n",
        "    ### TODO YOUR CODE STARTS ###\n",
        "    pts = None\n",
        "    ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "    return pts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e892980",
      "metadata": {},
      "source": [
        "### 2.3. Robot motions and wrist camera extrinsics (15 pts)\n",
        "\n",
        "When we merge the point clouds from the fixed cameras and wrist cameras, one problem remains: the wrist camera is mounted to the robot. It keeps relatively static to the robot's end effector (eef), rather than the calibration board. Therefore, as the robot moves, the extrinsics (world-to-camera) matrix for the wrist camera will change. In this sub-problem, we will complete the functions for calculating the new wrist camera extrinsics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74530967",
      "metadata": {},
      "source": [
        "**Robot trajectores**\n",
        "\n",
        "We save the robot trajectories in the ```data/recordings/robot``` directory. The following function reads the robot end effector's cartesian positions and orientations relative to the robot base, and returns the transformation matrix from the robot end effector to the robot base, given a frame index. The function is directly given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ab6871",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_eef_T_base(frame_idx):\n",
        "    with open(f'data/recording/robot/{frame_idx:06d}.json') as f:\n",
        "        data = json.load(f)\n",
        "    new_ee_pos = np.array(data['obs.ee_pos'])\n",
        "    new_ee_quat = np.array(data['obs.ee_quat'])\n",
        "    new_ee_rot = transforms3d.quaternions.quat2mat(new_ee_quat)\n",
        "    eef_T_base = np.eye(4)\n",
        "    eef_T_base[:3, :3] = new_ee_rot\n",
        "    eef_T_base[:3, 3] = new_ee_pos\n",
        "    return eef_T_base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23da0789",
      "metadata": {},
      "source": [
        "Now that we have the function to read the end effector motions, in the following function, we will calculate the wrist camera extrinsics (```world_to_new_wristcam```) given the old wrist camera extrinsics (```world_to_old_wristcam```) and the new robot pose (```new_eef_T_base```). To do this, we also need the old robot pose (```old_eef_T_base```) (i.e., the robot pose when the wrist camera calibration is done) and the robot base to calibration board transformation (```base_T_world```). These two transformations are constant, and we already hardcoded them into the function\n",
        "\n",
        "**Instructions**\n",
        "- Complete the function. (10 pts)\n",
        "- In the report, write the mathmatical formulations for claculating the new wristcam extrinsics. (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c450ff37",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_wristcam_extrinsics(world_T_old_wristcam, new_eef_T_base):\n",
        "    # world_T_old_wristcam: 4x4 old wrist camera extrinsics\n",
        "    # new_eef_T_base: 4x4 new end effector to robot base transformation\n",
        "    # return: 4x4 new wrist camera extrinsics\n",
        "\n",
        "    # these are hardcoded constants\n",
        "    old_eef_T_base = np.array([\n",
        "        [1.0, 0.0, 0.0, 0.2568],\n",
        "        [0.0, -1.0, 0.0, 0.0],\n",
        "        [0.0, 0.0, -1.0, 0.4005],\n",
        "        [0.0, 0.0, 0.0, 1.0],\n",
        "    ])\n",
        "    base_T_world = np.array(\n",
        "        [[1.0, 0.0, 0.0, -0.185],\n",
        "         [0.0, -1.0, 0.0, 0.115],\n",
        "         [0.0, 0.0, -1.0, -0.01],\n",
        "         [0.0, 0.0, 0.0, 1.0]]\n",
        "    )\n",
        "\n",
        "    ### TODO YOUR CODE STARTS ###\n",
        "    world_to_new_wristcam = None\n",
        "    ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "    return world_to_new_wristcam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a021c7c",
      "metadata": {},
      "source": [
        "### 2.4 Visualize (10 pts)\n",
        "\n",
        "Finally, we are ready to visualize the point clouds! We will use the recording data from the ```data/recordings/``` dir. The code will show the two rgb and depth images at frame 0, and launch an open3d interactive visualization of the merged point cloud. Feel free to play around with other frames as well.\n",
        "\n",
        "**Instructions**\n",
        "- Write the function calls to the previously defined functions. (7 pts) Note that \n",
        "  - The desired output from your code block is the N x 3 ```pts``` array (in world frame) and ```colors``` array (rgb values are already normalized to 0.0-1.0, and you do not need to further modify that).\n",
        "  - You might need to apply different extrinsics calculation for wrist camera (```name == 'wrist'```) and front camera (```name == 'front'```).\n",
        "- In the report, include a **screenshot** of the open3d point cloud visualization, and **answer** the question: from your final results, how can we visually verify that the calculated wrist camera extrinsics is correct? (E.g., what are some visual cues in the open3d visualization?) (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091c849f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def construct_merged_point_cloud(frame_idx, visualize=True):\n",
        "    names = ['front', 'wrist']\n",
        "\n",
        "    # load intrinsics and extrinsics\n",
        "    with open('data/calibration/intrinsics.json', 'r') as f:\n",
        "        intrinsics = json.load(f)\n",
        "    with open('data/calibration/extrinsics.json', 'r') as f:\n",
        "        extrinsics = json.load(f)\n",
        "\n",
        "    # to store the merged point cloud\n",
        "    pts_all = []\n",
        "    color_all = []\n",
        "\n",
        "    # iterate through each camera\n",
        "    for name in names:\n",
        "        rgb_path = f'./data/recording/camera_{name}/rgb/{frame_idx:06d}.jpg'\n",
        "        depth_path = f'./data/recording/camera_{name}/depth/{frame_idx:06d}.png'\n",
        "\n",
        "        # visualize first frame\n",
        "        color = cv2.imread(rgb_path)\n",
        "        color = cv2.cvtColor(color, cv2.COLOR_BGR2RGB)\n",
        "        color = color.astype(np.float32) / 255.0\n",
        "\n",
        "        # visualize color image\n",
        "        if visualize:\n",
        "            plt.figure(figsize=(6, 4), dpi=150)\n",
        "            plt.imshow(color)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        # read depth image\n",
        "        if name == 'front':\n",
        "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) / 1000.0  # mm to m\n",
        "        if name == 'wrist':\n",
        "            depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED) / 10000.0  # 0.1mm to m\n",
        "\n",
        "        # visualize depth image\n",
        "        if visualize:\n",
        "            depth_vis = cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "            depth_vis = cv2.applyColorMap(depth_vis, cv2.COLORMAP_VIRIDIS)\n",
        "            depth_vis = cv2.cvtColor(depth_vis, cv2.COLOR_BGR2RGB)\n",
        "            plt.figure(figsize=(6, 4), dpi=150)\n",
        "            plt.imshow(depth_vis)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        # convert depth and rgb images to point cloud (pts and colors)\n",
        "        ### TODO YOUR CODE STARTS ###\n",
        "        pts = None\n",
        "        color = None\n",
        "        ### TODO YOUR CODE ENDS ###\n",
        "\n",
        "        # filter points within bounding box for better visual quality\n",
        "        bbox = np.array([[-0.5, -0.5, -1.0], [0.8, 0.5, 0.2]])\n",
        "        pts_mask = np.logical_and.reduce((\n",
        "            pts[:, 0] >= bbox[0, 0], pts[:, 0] <= bbox[1, 0],\n",
        "            pts[:, 1] >= bbox[0, 1], pts[:, 1] <= bbox[1, 1],\n",
        "            pts[:, 2] >= bbox[0, 2], pts[:, 2] <= bbox[1, 2]\n",
        "        ))\n",
        "        pts = pts[pts_mask]\n",
        "        color = color[pts_mask]\n",
        "\n",
        "        pts_all.append(pts)\n",
        "        color_all.append(color)\n",
        "\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(np.ascontiguousarray(np.concatenate(pts_all, axis=0)).astype(np.float64))\n",
        "    pcd.colors = o3d.utility.Vector3dVector(np.ascontiguousarray(np.concatenate(color_all, axis=0)).astype(np.float64))\n",
        "    if visualize:\n",
        "        o3d.visualization.draw_geometries([pcd])\n",
        "    return pcd\n",
        "\n",
        "\n",
        "_ = construct_merged_point_cloud(frame_idx=0, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0291b9d",
      "metadata": {},
      "source": [
        "Here, we also provide an interesting dynamic video visualization of the point cloud. We reconstruct the point cloud for each frame and play the resulting point cloud sequence in real time using the open3d visualizer. You can see the entire process of the robot hanging the green mug on the rack, from the front camera view. Try the code! You can also change to the wrist camera view (how?), or implement any camera motions you like. You can include interesting results in the report, but it is not required and will not be graded.\n",
        "\n",
        "Note that the point cloud reconstruction could take a while.\n",
        "\n",
        "The desired visualization should look like:\n",
        "\n",
        "![image](data/vis/seq.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac57e6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def video_visualization(geoms):\n",
        "    vis = o3d.visualization.Visualizer()\n",
        "    vis.create_window()\n",
        "    vis.add_geometry(geoms[0])\n",
        "\n",
        "    ctr = vis.get_view_control()\n",
        "    params = ctr.convert_to_pinhole_camera_parameters()\n",
        "    with open('data/calibration/extrinsics.json', 'r') as f:\n",
        "        extrinsics = json.load(f)\n",
        "    E = np.array(extrinsics['front'], dtype=float)\n",
        "    params.extrinsic = E\n",
        "    ctr.convert_from_pinhole_camera_parameters(params, allow_arbitrary=True)\n",
        "\n",
        "    vis.poll_events()\n",
        "    vis.update_renderer()\n",
        "    \n",
        "    for g in geoms[1:]:\n",
        "        vis.clear_geometries()\n",
        "        vis.add_geometry(g)\n",
        "        ctr.convert_from_pinhole_camera_parameters(params, allow_arbitrary=True)\n",
        "        vis.poll_events()\n",
        "        vis.update_renderer()\n",
        "        time.sleep(0.03)\n",
        "    vis.destroy_window()\n",
        "\n",
        "pts_all_list = []\n",
        "for frame_idx in range(371):\n",
        "    print(f\"Processing frame {frame_idx}\")\n",
        "    pcd = construct_merged_point_cloud(frame_idx, visualize=False)\n",
        "    pts_all_list.append(pcd)\n",
        "\n",
        "video_visualization(pts_all_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4768409",
      "metadata": {},
      "source": [
        "## 3. Semantic Segmentation (24 pts)\n",
        "### 3.1 Visualize the Segmentation of Pens (17 pts)\n",
        "We provide four masks corresponding to the pens visible in the image. Please refer to the code in Section 3.2 to: (12 pts)\n",
        "1.\tProject the front-view RGB and depth images into a point cloud (PCD).\n",
        "2.\tLoad all four pen masks.\n",
        "3.\tMask the corresponding points, assign a distinct color to each pen, and visualize all of them.\n",
        "\n",
        "For the report, please include all intermediate visualizations along with descriptions of your implementation steps and any challenges you encountered during the process. (5 pts)\n",
        "\n",
        "The expected final visualization is shown below:\n",
        "\n",
        "![image.png](data/vis/example_output.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b5a8c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load RGB & Depth & 4 Masks\n",
        "name = \"front\"\n",
        "rgb_path = './data/recording/camera_front/rgb/000000.jpg'\n",
        "depth_path = './data/recording/camera_front/depth/000000.png'\n",
        "mask_paths = './data/vis/masks'\n",
        "\n",
        "### TODO Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecfdd17b",
      "metadata": {},
      "source": [
        "### 3.2 Geometric Distance Calculation (7 pts)\n",
        "\n",
        "After obtaining separate point clouds for each pen, assume each penâ€™s location is represented by the centroid of its point cloud. Please compute the closest and farthest pairwise distances among the pens. Additionally, clearly state which two pens (by color) are the closest and which two are the farthest apart. (5 pts)\n",
        "\n",
        "For the report, please include all intermediate calculation results and provide a description of what each calculation represents. (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b140710",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TODO Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b63666",
      "metadata": {},
      "source": [
        "## 4. Semantic Features (16 pts)\n",
        "\n",
        "In this section, you will learn how to extract DINO-V2 features from an image and write code to visualize a heatmap of feature similarity for a given query pixel. (12 pts)\n",
        "\n",
        "In your report, please include the visualizations, explain the final heatmap, and discuss your observations and findings. Additionally, select two more query points, generate their visualizations, and provide discussions on your findings for these points as well. (4 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408a4348",
      "metadata": {},
      "outputs": [],
      "source": [
        "rgb_path = './data/recording/camera_front/rgb/000000.jpg'\n",
        "rgb = cv2.imread(rgb_path)\n",
        "\n",
        "dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').eval()\n",
        "\n",
        "# Hyperparameters\n",
        "patch_size = 14\n",
        "\n",
        "# ensure input shape is compatible with dinov2\n",
        "H, W, _ = rgb.shape\n",
        "patch_h = int(H // patch_size)\n",
        "patch_w = int(W // patch_size)\n",
        "new_H = patch_h * patch_size\n",
        "new_W = patch_w * patch_size\n",
        "transformed_rgb = cv2.resize(rgb, (new_W, new_H))\n",
        "transformed_rgb = transformed_rgb.astype(np.float32) / 255.0 \n",
        "img_tensors = torch.tensor(transformed_rgb, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "# Use DINOv2 to extract pixel-wise features\n",
        "features_dict = dinov2.forward_features(img_tensors)\n",
        "raw_feature_grid = features_dict['x_norm_patchtokens']  # float32 [num_cams, patch_h*patch_w, feature_dim]\n",
        "raw_feature_grid = raw_feature_grid.reshape(1, patch_h, patch_w, -1)  # float32 [num_cams, patch_h, patch_w, feature_dim]\n",
        "\n",
        "# compute per-point feature using bilinear interpolation\n",
        "DINO_feature = interpolate(raw_feature_grid.permute(0, 3, 1, 2),  # float32 [num_cams, feature_dim, patch_h, patch_w]\n",
        "                                        size=(H, W),\n",
        "                                        mode='bilinear').permute(0, 2, 3, 1).squeeze(0)  # float32 [H, W, feature_dim]\n",
        "\n",
        "print(DINO_feature.shape)  # should be [H*W, feature_dim]\n",
        "print(rgb.shape)\n",
        "\n",
        "# Select a point on the image\n",
        "x, y = 470, 268 # Query Point\n",
        "plt.figure(figsize=(6, 4), dpi=150)\n",
        "plt.imshow(cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB))\n",
        "plt.scatter([y], [x], c='r', s=50)\n",
        "plt.title(f\"Query Point ({x}, {y})\")\n",
        "\n",
        "query_feature = np.array(DINO_feature[x, y, :].tolist(), dtype=np.float32)  # float32 [feature_dim]\n",
        "\n",
        "# Draw the heatmap figure to based on the cosine similarity\n",
        "### TODO Write your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49ccc08",
      "metadata": {},
      "source": [
        "## Submission checklist\n",
        "Include all the problems in your PDF report, and keep all the outputs of the notebook. A list of the question indices are:\n",
        "\n",
        "```1.1, 1.2```\n",
        "\n",
        "```2.1, 2.2, 2.3, 2.4```\n",
        "\n",
        "```3.1, 3.2```\n",
        "\n",
        "```4```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
